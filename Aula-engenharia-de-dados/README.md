# üìò Aula: Introdu√ß√£o √† Engenharia de Dados com Azure

**Dura√ß√£o:** 3 horas  
**P√∫blico-alvo:** Iniciantes (0 conhecimento pr√©vio)  
**Base:** Livro "Fundamentos da Engenharia de Dados"  
**Plataforma:** Microsoft Azure  
**Objetivo:** Apresentar de forma clara e pr√°tica os fundamentos da Engenharia de Dados e aplicar conceitos usando Azure SQL, Data Lake e Data Factory.

---

## üß≠ Roteiro Geral da Aula

| Bloco | Tema | Dura√ß√£o | Objetivo |
|-------|------|---------|----------|
| 1 | Abertura + Agenda + Introdu√ß√£o | 10 min | Engajar, alinhar expectativas |
| 2 | Diferen√ßas entre Engenharia, An√°lise e Ci√™ncia de Dados | 15 min | Entender pap√©is e responsabilidades |
| 3 | Fundamentos da Engenharia de Dados | 30 min | Teoria: ciclo de vida e elementos essenciais |
| 4 | Demonstra√ß√£o 1: Azure Data Lake + Azure SQL | 25 min | Mostrar armazenamento e ingest√£o |
| 5 | Demonstra√ß√£o 2: Azure Data Factory | 25 min | Mostrar transforma√ß√£o e orquestra√ß√£o |
| 6 | Hands-on Orientado (Lab ou Guiado) | 40 min | Atividade pr√°tica b√°sica no Azure |
| 7 | Conclus√£o + Dicas + Perguntas | 15 min | Recapitular e abrir para d√∫vidas |

---

## üë• Diferen√ßa entre Engenharia, An√°lise e Ci√™ncia de Dados (15 min)

| Papel | Foco | Entreg√°vel | Ferramentas |
|-------|------|------------|-------------|
| **Engenheiro de Dados** | Infraestrutura e pipelines | Bases estruturadas e acess√≠veis | Spark, SQL, Data Factory |
| **Analista de Dados** | Explora√ß√£o e relat√≥rios | Dashboards e KPIs | Power BI, Excel, SQL |
| **Cientista de Dados** | Modelagem preditiva | Modelos e previs√µes | Python, ML, Jupyter |

> üí° **Resumo:**  
> O engenheiro prepara e entrega os dados.  
> O analista consome e interpreta.  
> O cientista modela e prev√™.

---

## ‚öôÔ∏è Fundamentos da Engenharia de Dados (30 min)

### üîÅ Ciclo de Vida

1. **Gera√ß√£o de dados**: sistemas, sensores, APIs
2. **Armazenamento**: Data Lakes, bancos relacionais
3. **Ingest√£o**: movimentar dados (batch, streaming)
4. **Transforma√ß√£o**: limpeza, padroniza√ß√£o, enriquecimento
5. **Disponibiliza√ß√£o**: camadas de consumo (BI, APIs)

### üîß Elementos Subjacentes

- **Seguran√ßa**: controle de acesso, criptografia, compliance
- **Engenharia de Dados como Disciplina**: versionamento, testes
- **DataOps**: automa√ß√£o, deploy, CI/CD
- **Arquitetura de Dados**: modular, escal√°vel, eficiente
- **Orquestra√ß√£o**: controle de fluxo e depend√™ncias
- **Engenharia de Software**: c√≥digo limpo, fun√ß√µes reutiliz√°veis

---

## üíª Demonstra√ß√£o 1 ‚Äì Azure Data Lake +  SQL (25 min)

### Objetivo:
Mostrar armazenamento e infra do sql

> üí° Conceitos explicados: Data Lake como Bronze, Azure SQL como Silver/Gold

### Scripts SQL e Explica√ß√µes para PostgreSQL
**Objetivo:** Reunir scripts SQL completos para criar tabelas baseadas no dataset Olist, com chaves prim√°rias, √≠ndices e orienta√ß√µes de carga.

## Conte√∫do

1. Recomenda√ß√µes e Fluxo de Carga
2. `create_db.sql`
3. `schema.sql` ‚Äî cria√ß√£o de tabelas com PKs
4. `indexes.sql` ‚Äî √≠ndices secund√°rios
5. `foreign_keys.sql` ‚Äî chaves estrangeiras
6. `load_data.sql` ‚Äî carregamento de dados
7. Observa√ß√µes finais

---

## 1) Recomenda√ß√µes e Fluxo de Carga

- Criar base e schema primeiro.
- Criar tabelas apenas com PKs.
- Carregar dados via `\copy` ou `COPY`.
- Criar √≠ndices depois da carga.
- Adicionar FKs apenas se necess√°rio.
- Rodar `VACUUM ANALYZE` no final.

---

## 2) `create_db.sql`

```sql
CREATE DATABASE olist_db WITH ENCODING='UTF8' LC_COLLATE='pt_BR.UTF-8' LC_CTYPE='pt_BR.UTF-8' TEMPLATE=template0;
-- \c olist_db
CREATE SCHEMA IF NOT EXISTS public;
```

---

## 3) `schema.sql`

```sql
CREATE TABLE olist.olist_customers (
  customer_id TEXT PRIMARY KEY,
  customer_unique_id TEXT,
  customer_zip_code_prefix INTEGER,
  customer_city TEXT,
  customer_state TEXT
);

CREATE TABLE olist.olist_geolocation (
  geolocation_zip_code_prefix INTEGER,
  geolocation_lat DOUBLE PRECISION,
  geolocation_lng DOUBLE PRECISION,
  geolocation_city TEXT,
  geolocation_state TEXT,
  PRIMARY KEY (geolocation_zip_code_prefix, geolocation_lat, geolocation_lng)
);

CREATE TABLE olist.olist_order_items (
  order_id TEXT,
  order_item_id INTEGER,
  product_id TEXT,
  seller_id TEXT,
  shipping_limit_date TIMESTAMP,
  price NUMERIC(12,2),
  freight_value NUMERIC(12,2),
  PRIMARY KEY (order_id, order_item_id)
);

CREATE TABLE olist.olist_order_payments (
  order_id TEXT,
  payment_sequential INTEGER,
  payment_type TEXT,
  payment_installments INTEGER,
  payment_value NUMERIC(12,2),
  PRIMARY KEY (order_id, payment_sequential)
);

CREATE TABLE olist.olist_order_reviews (
  review_id TEXT PRIMARY KEY,
  order_id TEXT,
  review_score SMALLINT,
  review_comment_title TEXT,
  review_comment_message TEXT,
  review_creation_date TIMESTAMP,
  review_answer_timestamp TIMESTAMP
);

CREATE TABLE olist.olist_orders (
  order_id TEXT PRIMARY KEY,
  customer_id TEXT,
  order_status TEXT,
  order_purchase_timestamp TIMESTAMP,
  order_approved_at TIMESTAMP,
  order_delivered_carrier_date TIMESTAMP,
  order_delivered_customer_date TIMESTAMP,
  order_estimated_delivery_date TIMESTAMP
);

CREATE TABLE olist.olist_products (
  product_id TEXT PRIMARY KEY,
  product_category_name TEXT,
  product_name_lenght INTEGER,
  product_description_lenght INTEGER,
  product_photos_qty INTEGER,
  product_weight_g INTEGER,
  product_length_cm INTEGER,
  product_height_cm INTEGER,
  product_width_cm INTEGER
);

CREATE TABLE olist.olist_sellers (
  seller_id TEXT PRIMARY KEY,
  seller_zip_code_prefix INTEGER,
  seller_city TEXT,
  seller_state TEXT
);

CREATE TABLE olist.product_category_name_translation (
  product_category_name TEXT PRIMARY KEY,
  product_category_name_english TEXT
);
```
Rodar pipeline do hop para popular as tabelas, com esse create table vai dar erro em 2 tabelas, que devem ser corrigidas, mostrando a impoportancia da chave primaria e dos tipos de dados corretos:
<img width="1391" height="994" alt="image" src="https://github.com/user-attachments/assets/72d5779e-1361-48a8-b200-9fc361e4c5ac" />

Rodar:
```
-- 1. Remove a constraint de PK atual
ALTER TABLE public.olist_geolocation
  DROP CONSTRAINT olist_geolocation_pkey;

-- 2. Cria uma nova PK com todas as colunas
ALTER TABLE public.olist_geolocation
  ADD CONSTRAINT olist_geolocation_pkey
  PRIMARY KEY (
    geolocation_zip_code_prefix,
    geolocation_lat,
    geolocation_lng,
    geolocation_city,
    geolocation_state
  );
```
Retirar lazy conversion do olist_order_review:
<img width="1246" height="591" alt="image" src="https://github.com/user-attachments/assets/058b8e49-456d-40af-a1b3-4ed23bf78c03" />

Depois ordenar e retirar duplicatas da geolocaliza√ß√£o:
<img width="500" height="192" alt="image" src="https://github.com/user-attachments/assets/08ba4a84-244c-41a6-afb3-8e4c01df105a" />




---

## 4) `indexes.sql`

```sql
CREATE INDEX idx_customers_unique_id ON public.olist_customers (customer_unique_id);
CREATE INDEX idx_customers_zip ON public.olist_customers (customer_zip_code_prefix);
CREATE INDEX idx_geo_city_state ON public.olist_geolocation (geolocation_city, geolocation_state);
CREATE INDEX idx_order_items_product_id ON public.olist_order_items (product_id);
CREATE INDEX idx_order_items_seller_id ON public.olist_order_items (seller_id);
CREATE INDEX idx_order_payments_type ON public.olist_order_payments (payment_type);
CREATE INDEX idx_order_reviews_order_id ON public.olist_order_reviews (order_id);
CREATE INDEX idx_orders_customer_id ON public.olist_orders (customer_id);
CREATE INDEX idx_orders_purchase_ts ON public.olist_orders (order_purchase_timestamp);
CREATE INDEX idx_products_category ON public.olist_products (product_category_name);
CREATE INDEX idx_sellers_zip ON public.olist_sellers (seller_zip_code_prefix);
```

---

## 5) `foreign_keys.sql`

```sql
ALTER TABLE public.olist_orders ADD CONSTRAINT fk_orders_customers FOREIGN KEY (customer_id) REFERENCES public.olist_customers (customer_id);
ALTER TABLE public.olist_order_items ADD CONSTRAINT fk_order_items_orders FOREIGN KEY (order_id) REFERENCES public.olist_orders (order_id);
ALTER TABLE public.olist_order_items ADD CONSTRAINT fk_order_items_products FOREIGN KEY (product_id) REFERENCES public.olist_products (product_id);
ALTER TABLE public.olist_order_items ADD CONSTRAINT fk_order_items_sellers FOREIGN KEY (seller_id) REFERENCES public.olist_sellers (seller_id);
ALTER TABLE public.olist_order_payments ADD CONSTRAINT fk_payments_orders FOREIGN KEY (order_id) REFERENCES public.olist_orders (order_id);
ALTER TABLE public.olist_order_reviews ADD CONSTRAINT fk_reviews_orders FOREIGN KEY (order_id) REFERENCES public.olist_orders (order_id);
ALTER TABLE public.olist_products ADD CONSTRAINT fk_products_category_translation FOREIGN KEY (product_category_name) REFERENCES public.product_category_name_translation (product_category_name);
```

---

## 6) `load_data.sql`

```sql
COPY public.olist_customers FROM '/path/to/olist_customers_dataset.csv' CSV HEADER;
COPY public.olist_orders FROM '/path/to/olist_orders_dataset.csv' CSV HEADER;
```

**No cliente psql:**

```sh
\copy public.olist_customers FROM 'local/path/olist_customers_dataset.csv' CSV HEADER
\copy public.olist_orders FROM 'local/path/olist_orders_dataset.csv' CSV HEADER
```

---

## 7) Observa√ß√µes finais

- IDs como `TEXT` por serem hashes.
- `NUMERIC(12,2)` para valores monet√°rios.
- Criar √≠ndices somente ap√≥s carga massiva.
- Validar contagem de registros antes de FKs.
---

## üîÑ Demonstra√ß√£o 2 ‚Äì Azure Data Factory (25 min)

### Objetivo:
Mostrar orquestra√ß√£o e transforma√ß√£o

### Etapas:
- Criar pipeline no **Azure Data Factory**
  - Fonte: Data Lake (CSV) ou API
  - Destino: Postgres SQL
- Configurar Linked Services, Datasets e Activities
- Executar e monitorar

> üí° Pipeline simples mas did√°tico mostrando ETL no Azure

---

## üõ†Ô∏è Hands-on Orientado (40 min)

<img width="2486" height="1496" alt="image" src="https://github.com/user-attachments/assets/b8ee926d-70d1-43a7-a5b5-6c2aaec74c92" />
> ### Brazilian E-Commerce Public Dataset by Olist
> https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce

```python
# Install dependencies as needed:
# pip install kagglehub[pandas-datasets]
import kagglehub
from kagglehub import KaggleDatasetAdapter

# Set the path to the file you'd like to load
file_path = ""

# Load the latest version
df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "olistbr/brazilian-ecommerce",
  file_path,
  # Provide any additional arguments like 
  # sql_query or pandas_kwargs. See the 
  # documenation for more information:
  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
)

print("First 5 records:", df.head())
```

### Tarefa: Executar como tutorial guiado caso os alunos n√£o tenham acesso.
1. Criar container no Data Lake
2. Fazer upload de CSV ou extrair via API Kaggle
3. Criar pipeline no ADF
4. Ingerir dados para Postgres SQL
5. Verificar resultado via query

---

## ‚úÖ Conclus√£o e Recomenda√ß√µes (15 min)

### Recapitula√ß√£o:
- Diferen√ßas entre os pap√©is de dados
- Ciclo de vida da Engenharia de Dados
- Ferramentas usadas: Data Lake, SQL, Data Factory
- Demonstra√ß√µes pr√°ticas realizadas

### üìö Materiais para Estudo:
- Livro **"Fundamentos da Engenharia de Dados"**
- Curso Microsoft Learn ‚Äì **Azure Fundamentals**
- Curso gratuito: **DP-900** (Azure Data Fundamentals)

---

## üßæ Materiais de Apoio

- Slides com resumo te√≥rico (PDF ou PPT)
- Scripts SQL utilizados
- JSON de pipeline do ADF (export√°vel)
- Dataset CSV de exemplo
- Passo a passo das demonstra√ß√µes

---

### ‚úâÔ∏è Frase final:

> ‚ÄúEngenharia de dados √© o que torna poss√≠vel transformar dados brutos em decis√µes valiosas. Sem estrutura, n√£o h√° valor.‚Äù

